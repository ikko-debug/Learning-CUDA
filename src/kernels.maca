#include <algorithm>
#include <cmath>
#include <vector>
#include <common/maca_fp16.h>

#include "../tester/utils.h"

#define BLOCK_SIZE 256
constexpr int Br = 16;
constexpr int Bc = 16;




template <typename T>
__device__ __forceinline__ T warpReduceSum(T val) {
    for (int offset = 32 / 2; offset > 0; offset >>= 1) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

template <typename T>
__device__ __forceinline__ T blockReduceSum(T val) {
    static __shared__ T shared[32]; 
    int lane = threadIdx.x % 32; 
    int wid  = threadIdx.x / 32; 

        const T* __restrict__ Q,
        const T* __restrict__ K,
        const T* __restrict__ V,
        T* __restrict__ O,
        int batch_size,
        int target_seq_len,
        int src_seq_len,
        int query_heads,
        int kv_heads,
        int head_dim,
        bool is_causal,
        float scale) {
        //线程布局: x 维对应列(Bc)，y 维对应行(Br)
        int tx = threadIdx.x; //Bc 维度
        int ty = threadIdx.y; //Br 维度

        int batch_idx = blockIdx.z;
        int head_idx = blockIdx.y;
        int q_block_idx = blockIdx.x;

        //当前 block 负责的 query 行区间
        int q_start_idx = q_block_idx * Br;
        int q_len_local = min(Br, target_seq_len - q_start_idx);

        int kv_head_idx = (head_idx * kv_heads) / query_heads;

        //共享内存布局：Q tile(Br*D), K tile(Bc*D), V tile(Bc*D), O accum(Br*D), m(Br), l(Br)
        extern __shared__ float smem[];
        float* s_Q = smem;                                 //Br * head_dim
        float* s_K = s_Q + Br * head_dim;                  //Bc * head_dim
        float* s_V = s_K + Bc * head_dim;                  //Bc * head_dim
        float* s_O = s_V + Bc * head_dim;                  //Br * head_dim
        float* s_m = s_O + Br * head_dim;                  //Br
        float* s_l = s_m + Br;                             //Br

        //1 载入 Q tile，初始化输出累加器与统计量
        for (int i = threadIdx.y * blockDim.x + threadIdx.x; i < Br * head_dim; i += blockDim.x * blockDim.y) {
            int r = i / head_dim;
            int c = i % head_dim;
            int global_q = q_start_idx + r;
            if (r < q_len_local && global_q < target_seq_len) {
                size_t q_index = ((static_cast<size_t>(batch_idx) * target_seq_len + global_q) * query_heads + head_idx) * head_dim + c;
                s_Q[r * head_dim + c] = to_float(Q[q_index]);
            } else {
                s_Q[r * head_dim + c] = 0.0f;
            }
            s_O[r * head_dim + c] = 0.0f;
        }
        if (tx == 0 && ty < Br) {
            s_m[ty] = -1e20f;
            s_l[ty] = 0.0f;
        }
        __syncthreads();

        for (int j_base = 0; j_base < src_seq_len; j_base += Bc) {
            //2 逐块加载 K/V tile
            int kv_len_local = min(Bc, src_seq_len - j_base);

            for (int i = threadIdx.y * blockDim.x + threadIdx.x; i < Bc * head_dim; i += blockDim.x * blockDim.y) {
                int r = i / head_dim;
                int c = i % head_dim;
                int global_k = j_base + r;
                if (r < kv_len_local && global_k < src_seq_len) {
                    size_t k_index = ((static_cast<size_t>(batch_idx) * src_seq_len + global_k) * kv_heads + kv_head_idx) * head_dim + c;
                    s_K[r * head_dim + c] = to_float(K[k_index]);
                    s_V[r * head_dim + c] = to_float(V[k_index]);
                } else {
                    s_K[r * head_dim + c] = 0.0f;
                    s_V[r * head_dim + c] = 0.0f;
                }
            }
            __syncthreads();

            //3 对每个 query 行做在线 softmax 更新并累加输出
            if (tx == 0 && ty < q_len_local) {
                for (int tk = 0; tk < kv_len_local; ++tk) {
                    int global_q_idx = q_start_idx + ty;
                    int global_k_idx = j_base + tk;
                    if (is_causal && global_k_idx > global_q_idx) {
                        continue;
                    }

                    float dot = 0.0f;
                    for (int d = 0; d < head_dim; ++d) {
                        dot += s_Q[ty * head_dim + d] * s_K[tk * head_dim + d];
                    }
                    dot *= scale;

                    float m_prev = s_m[ty];
                    float l_prev = s_l[ty];
                    float m_new = fmaxf(m_prev, dot);
                    float p_prev = expf(m_prev - m_new);
                    float p_curr = expf(dot - m_new);
                    float l_new = l_prev * p_prev + p_curr;

                    for (int d = 0; d < head_dim; ++d) {
                        s_O[ty * head_dim + d] = s_O[ty * head_dim + d] * p_prev + p_curr * s_V[tk * head_dim + d];
                    }

                    s_m[ty] = m_new;
                    s_l[ty] = l_new;
                }
            }

            __syncthreads();
        }

        //4) 归一化并写回输出
        if (ty < q_len_local) {
            float denom = s_l[ty];
            float inv_l = (denom > 0.0f) ? (1.0f / denom) : 0.0f;
            for (int d = tx; d < head_dim; d += Bc) {
                float val = s_O[ty * head_dim + d] * inv_l;
                int global_q = q_start_idx + ty;
                size_t o_index = ((static_cast<size_t>(batch_idx) * target_seq_len + global_q) * query_heads + head_idx) * head_dim + d;
                O[o_index] = from_float<T>(val);
            }
        }
    int qh = tmp % query_heads;
    tmp /= query_heads;
    int t = tmp % target_seq_len;
    int b = tmp / target_seq_len;
    int kv_h = (qh * kv_heads) / query_heads;

    //1) 将当前 Block 负责的 Q 向量加载到寄存器
    float q_val = to_float(Q[q_idx * head_dim + tid]);
    float m_i = -INFINITY;
    float l_i = 0.0f;
    float o_i = 0.0f;

    //共享内存布局: K(head_dim) + V(head_dim) + dot(1) + reduction(head_dim)
    extern __shared__ float s_mem[];
    float* s_k = s_mem; 
    float* s_v = s_mem + head_dim; 
    float* s_dot = s_mem + 2 * head_dim; 
    float* s_red = s_mem + 2 * head_dim + 1; 

    for (int j = 0; j < src_seq_len; ++j) {
        if (is_causal && j > t) continue;

        const T* k_ptr = K + (((b * src_seq_len + j) * kv_heads + kv_h) * head_dim);
        const T* v_ptr = V + (((b * src_seq_len + j) * kv_heads + kv_h) * head_dim);

        //2) 加载 K 和 V 到 Shared Memory
        s_k[tid] = to_float(k_ptr[tid]);
        s_v[tid] = to_float(v_ptr[tid]);
        __syncthreads();

        //3) 计算点积 S = Q * K^T
        float score = q_val * s_k[tid];
        s_red[tid] = score;
        __syncthreads();
        for (int stride = (head_dim + 1) / 2; stride > 0; stride = (stride == 1) ? 0 : (stride + 1) / 2) {
            if (tid < stride && (tid + stride) < head_dim) {
                s_red[tid] += s_red[tid + stride];
            }
            __syncthreads();
        }

        if (tid == 0) s_dot[0] = s_red[0] * scale;
        __syncthreads();

        float dot = s_dot[0];
        //4) Online Softmax 更新 + 输出累加
        float m_prev = m_i;
        m_i = fmaxf(m_prev, dot);
        float p_prev = expf(m_prev - m_i);
        float p_curr = expf(dot - m_i);
        
        float l_prev = l_i;
        l_i = l_prev * p_prev + p_curr;
        o_i = (o_i * l_prev * p_prev + p_curr * s_v[tid]) / l_i;
        __syncthreads();
    }

    //5) 写回输出
    O[q_idx * head_dim + tid] = from_float<T>(o_i);
}



template <typename T>
T trace(const std::vector<T>& h_input, size_t rows, size_t cols) {
    if (rows == 0 || cols == 0) return T(0);
    size_t n = std::min(rows, cols);
    size_t total_elems = rows * cols;

    T *d_input, *d_out;
    // 使用 macaMalloc 代替 cudaMalloc
    RUNTIME_CHECK(macaMalloc(&d_input, total_elems * sizeof(T)));
    RUNTIME_CHECK(macaMalloc(&d_out, sizeof(T)));
    RUNTIME_CHECK(macaMemcpy(d_input, h_input.data(), total_elems * sizeof(T), macaMemcpyHostToDevice));
    RUNTIME_CHECK(macaMemset(d_out, 0, sizeof(T)));

    traceKernel<T><<< (n + BLOCK_SIZE - 1) / BLOCK_SIZE, BLOCK_SIZE >>>(d_input, rows, cols, n, d_out);
    RUNTIME_CHECK(macaGetLastError());
    RUNTIME_CHECK(macaDeviceSynchronize());

    T h_out;
    RUNTIME_CHECK(macaMemcpy(&h_out, d_out, sizeof(T), macaMemcpyDeviceToHost));
    RUNTIME_CHECK(macaFree(d_input));
    RUNTIME_CHECK(macaFree(d_out));
    return h_out;
}

template <typename T>
void flashAttention(const std::vector<T>& h_q, const std::vector<T>& h_k,
                    const std::vector<T>& h_v, std::vector<T>& h_o,
                    int batch_size, int target_seq_len, int src_seq_len, 
                    int query_heads, int kv_heads, int head_dim, bool is_causal) {
    
    size_t q_elems = batch_size * target_seq_len * query_heads * head_dim;
    size_t k_elems = batch_size * src_seq_len * kv_heads * head_dim;
    h_o.resize(q_elems);

    T *d_q, *d_k, *d_v, *d_o;
    RUNTIME_CHECK(macaMalloc(&d_q, q_elems * sizeof(T)));
    RUNTIME_CHECK(macaMalloc(&d_k, k_elems * sizeof(T)));
    RUNTIME_CHECK(macaMalloc(&d_v, k_elems * sizeof(T)));
    RUNTIME_CHECK(macaMalloc(&d_o, q_elems * sizeof(T)));

    RUNTIME_CHECK(macaMemcpy(d_q, h_q.data(), q_elems * sizeof(T), macaMemcpyHostToDevice));
    RUNTIME_CHECK(macaMemcpy(d_k, h_k.data(), k_elems * sizeof(T), macaMemcpyHostToDevice));
    RUNTIME_CHECK(macaMemcpy(d_v, h_v.data(), k_elems * sizeof(T), macaMemcpyHostToDevice));
    RUNTIME_CHECK(macaMemset(d_o, 0, q_elems * sizeof(T)));

    //线程块: (Bc, Br) 形成 [列, 行] 的 tile 计算
    dim3 block(Bc, Br);
    int grid_x = (target_seq_len + Br - 1) / Br;
    dim3 grid(grid_x, query_heads, batch_size);
    //共享内存大小与 kernel 中的 smem 布局保持一致
    size_t shared_bytes = (Br * head_dim + Bc * head_dim + Bc * head_dim + Br * head_dim + Br + Br) * sizeof(float);
    float scale = 1.0f / sqrtf((float)head_dim);

    flash_attention_v1_kernel<T><<<grid, block, shared_bytes>>>(
        d_q, d_k, d_v, d_o, batch_size, target_seq_len, src_seq_len, 
        query_heads, kv_heads, head_dim, is_causal, scale);
    RUNTIME_CHECK(macaGetLastError());
    RUNTIME_CHECK(macaDeviceSynchronize());

    RUNTIME_CHECK(macaMemcpy(h_o.data(), d_o, q_elems * sizeof(T), macaMemcpyDeviceToHost));

    RUNTIME_CHECK(macaFree(d_q));
    RUNTIME_CHECK(macaFree(d_k));
    RUNTIME_CHECK(macaFree(d_v));
    RUNTIME_CHECK(macaFree(d_o));
}


template int trace<int>(const std::vector<int>&, size_t, size_t);
template float trace<float>(const std::vector<float>&, size_t, size_t);
template void flashAttention<float>(const std::vector<float>&, const std::vector<float>&, const std::vector<float>&, std::vector<float>&, int, int, int, int, int, int, bool);
template void flashAttention<half>(const std::vector<half>&, const std::vector<half>&, const std::vector<half>&, std::vector<half>&, int, int, int, int, int, int, bool);